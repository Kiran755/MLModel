# -*- coding: utf-8 -*-
"""Credit Card Fraud Detection using ML models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kf3KlGyDe74TZFrQGPwsr4a7L5ICLSPn

## Credit Card Fraud Detection:
This dataset is used to detect the credit card fraud detection. This is a classification problem. This is an imbalanced dataset based on target variable. So In this Project, I will use encoding and decording techniques to balanced dataset.

## About Dataset:
Digital payments are evolving, but so are cyber crimes.
According to the Data Breach Index, more than 5 million records are being stolen on a daily basis, a concerning statistic that shows - fraud is still very common both for Card-Present and Card-not Present type of payments.
In today’s digital world where trillions of Card transaction happens per day, detection of fraud is challenging.
This Dataset sourced by some unnamed institute.

## Problem Statement:
* The problem statement chosen for this project is to predict fraudulent credit card transactions with the help of machine learning models.
* In this project, we will analyse customer-level data which has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group.
* The dataset is taken from the Kaggle website and it has a total of 1000,000 transactions,out of which 87403 are fraudulent. Since the dataset is imbalanced, so it needs to be handled before model building.

## Challenges:
This dataset is used to detect the credit card fraud detection. This is a classification problem. This is an imbalanced dataset based on target variable. So In this Project, I will use encoding and decording techniques to balanced dataset.

These are various techniques as follows -

* Logistics Regression
* Random Forest
* Decision Tree
* KNN
* NaiveBias

## Libraries:
* Numpy : for numerical computing (https://numpy.org/doc/stable/reference/?v=20230420065146)

* Pandas: for data manipulation and analysis (https://pandas.pydata.org/pandas-docs/stable/?v=20230420065146)

* Matplotlib and Seaborn: for data visualization (https://matplotlib.org/stable/users/index.html ,https://seaborn.pydata.org/#:~:text=Seaborn%20is%20a%20Python%20data,attractive%20and%20informative%20statistical%20graphics)

* Sklearn: It is a popular machine learning library in Python that provides a wide range of tools for data preprocessing, feature selection, model training, and model evaluation.(https://scikit-learn.org/stable/)

## Project Pipeline:
The project pipeline can be briefly summarized in the following steps:

#### Data Understanding:
Here, we need to load the data and understand the features present in it. This would help us choose the features that we will need for your final model.
#### Exploratory data analytics (EDA):
Normally, in this step, we need to perform univariate and bivariate analyses of the data, followed by feature transformations, if necessary. For the current data set, because Gaussian variables are used, we do not need to perform Z-scaling. However, you can check if there is any skewness in the data and try to mitigate it, as it might cause problems during the model-building phase.
##### Train/Test Split:
Now we are familiar with the train/test split, which we can perform in order to check the performance of our models with unseen data. Here, for validation, we can use the k-fold cross-validation method. We need to choose an appropriate k value so that the minority class is correctly represented in the test folds.
#### Model-Building/Hyperparameter Tuning:
This is the final step at which we can try different models and fine-tune their hyperparameters until we get the desired level of performance on the given dataset. We should try and see if we get a better model by the various sampling techniques.
#### Model Evaluation:
We need to evaluate the models using appropriate evaluation metrics. Note that since the data is imbalanced it is is more important to identify which are fraudulent transactions accurately than the non-fraudulent. We need to choose an appropriate evaluation metric which reflects this business goal.

## Importing Libraries and Dataset:
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import warnings
warnings.filterwarnings("ignore")

data=pd.read_csv("card_transdata.csv")
data

"""## Exploratory Data Analysis (EDA):

* There are total of 8 columns: 3 continous, 4 categorical,and 1 target column
* There are 1000000 rows
* Fraud is the target variable
* No missing values
"""

data.shape

data.columns

"""## Top 5 values"""

data.head()

"""## last 5 values"""

data.tail()

"""## Feature Explanation:
distance_from_home - the distance from home where the transaction happened.

distance_from_last_transaction - the distance from last transaction happened.

ratio_to_median_purchase_price - Ratio of purchased price transaction to median purchase price.

repeat_retailer - Is the transaction happened from same retailer.

used_chip - Is the transaction through chip (credit card).

used_pin_number - Is the transaction happened by using PIN number.

online_order - Is the transaction an online order.

fraud - Is the transaction fraudulent.

## Finding the information of the data:
"""

data.info()

"""## Finding the Null Values in the Dataset:"""

data.isnull()

data.isnull().sum()

"""There are no null values in this datset.

## Handling duplicated values:
"""

data.duplicated().sum()

data.shape

"""##  Basic statistics:"""

data.describe()

"""###  Outliers treatments:"""

sns.catplot(x="fraud",kind="boxen",data=data)   ##box graph
plt.show()

y = data['fraud']
removed_outliers = y.between(y.quantile(.05), y.quantile(.95))
removed_outliers

print(removed_outliers.value_counts())

"""There are no outliers.

## Value count of columns:
"""

for i in data.columns:
  print(i, len(data[i].value_counts().index))

UsedChip = data['used_chip'].value_counts()

UsedChip.to_frame()

UsedPin=data['used_pin_number'].value_counts()

UsedPin.to_frame()

Fraud=data['fraud'].value_counts()
Fraud.to_frame()

order=data['online_order'].value_counts()
order.to_frame()

"""## Unique values of every columns:"""

data.nunique()

"""## Data Insights:
In the given dataset, There are 87403 frauds which is 8.70% of given dataset.
"""

fraud_c=pd.DataFrame(data["fraud"].value_counts())
fraud_c

"""## "Categorical Variables"
 * Repeat Retailer: Most of the time transitions are in the same retailer.
 * Used Chip: Most of the time transitions are not using the chip, but we have a considerable number of transitions using the chip.
 * Used Pin Number: Most of the time transitions are not using the Pin Number.
 * Online Order: We have more Online Orders than Fisical.
 * Fraud: We have a few frauds in our database comparing to non frauds.

## Data Visualisation:
"""



plt.pie(fraud_c["fraud"],labels=['genuine','fraud'],autopct='%.1f%%', explode=(0.2,0))
plt.show()

"""The dataset is heavily imbalanced. As it can be seen from the charts, number of fraud transactions are significantly low when compared to non-fraud transactions.

### What Percent of Fraud Transactions Are Online?
"""

plt.pie(data["online_order"].value_counts(),labels=["Online","Offline"],autopct='%.1f%%', explode=(0.2,0))
plt.title("What Percent of Fraud Transactions Are Online?")
plt.show()

"""It shows maximum frauds have done by online which is 65.10% of given dataset.

### What Percentage of  frauds  happened using Pins?
"""

plt.pie(data['used_pin_number'].value_counts(),labels=["Without Pin","Pin"],autopct='%.1f%%', explode=(0.2,0))
plt.title("What Percentage of frauds happened using Pins?")
plt.show()

order=data['repeat_retailer'].value_counts()
order.to_frame()

plt.pie(data["repeat_retailer"].value_counts(),labels=['Yes','No'],autopct='%.1f%%', explode=(0.1,0))
plt.show()

"""## Bivariate Analysis:
When we compare our variable target with others categorical variables, we can see some insights:

Most of the frauds are in the same retailer in a online purchase, without using the chip and without using the pin.¶
"""

plt.figure(figsize = (15,12))

plt.subplot(2,2,1)
sns.countplot(x = 'fraud', hue= 'repeat_retailer', data = data)

plt.subplot(2,2,2)
sns.countplot(x = 'fraud', hue= 'used_chip', data = data)

plt.subplot(2,2,3)
sns.countplot(x = 'fraud', hue= 'used_pin_number', data = data)

plt.subplot(2,2,4)
sns.countplot(x = 'fraud', hue= 'online_order', data = data)
plt.show()

"""### Continuous Variables:
* Distance from home: Most of the time transitions are close to home.
* Distance from last transition: Most of the time transitions are close to the last transition.
* Ratio to Median Purchase time: Most of the time transitions are not much diferent than average.
"""

sns.catplot(x = "fraud", y = "distance_from_home", data = data)
plt.show()

"""When we compare our variable target with the variable Distance From Home we can see that we don't have a big difference, it's almost a same pattern.¶"""

sns.catplot(x = "fraud", y = "distance_from_last_transaction",  data = data)
plt.show()

"""When we compare our variable target with the variable Distance From Last Transiction we can see that we don't have a big difference, it's almost a same pattern,"""

sns.catplot(x = "fraud", y = "ratio_to_median_purchase_price",  data = data)
plt.show()

"""When we compare our target variable with the Ratio to Median Purchase Time variable we can see that we have more frauds when the purchase value is far from the Median.¶"""

sns.scatterplot(x='distance_from_home', y='distance_from_last_transaction', data=data, hue='fraud')
plt.show()

"""## Making Dataset small just for Visualisation:"""

data_new = data.sample(n=10000, random_state=42)

sns.pairplot(data_new ,hue='fraud')
plt.show()

"""## Correlation:"""

data.corr()

"""Verifying the correlation between our variables, here we can see that we don't have a strong correlation.¶"""

plt.figure(figsize = (15,10))
sns.heatmap(data.corr(), annot=True)

plt.show()

"""Most correlation values are very close to 0, which indicates that our features are weakly correlated

##### Putting feature variables into X
"""

X=data.drop(["fraud"], axis = 1)

"""##### Putting target variable to y"""

Y=data["fraud"]

"""## Splitting the Dataset into Train & Test:"""

# Import library
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)

print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)

"""## Logistic Regression:"""

from sklearn.linear_model import LogisticRegression

# Impoting metrics
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import classification_report

model1=LogisticRegression()

model1.fit(X_train,Y_train)

y_pred=model1.predict(X_test)

model1.score(X_train,Y_train)*100

model1.score(X_test,Y_test)*100

accuracy1=accuracy_score(Y_test, y_pred)*100
print('\nAccuracy for Logistic Regression =',accuracy1)
print('\n\nClassification_report :-\n\n',classification_report(Y_test, y_pred))
print('\n\nConfusion_Matrix : \n')
plot_confusion_matrix(model1,X_test,Y_test)
plt.show()

"""#### Pretty nice! detected 95.7% of the fraudulent transactions! But also falsely classified 4.30% of the non-fraud transactions as fraudulent. Let's use other method to find a more balanced threshold.

## DecisionTreeClassifier:
"""

from sklearn.tree import DecisionTreeClassifier

model2=DecisionTreeClassifier()

model2.fit(X_train,Y_train)

y_pred = model2.predict(X_test)

accuracy2=accuracy_score(Y_test, y_pred)*100
accuracy2

print('\nAccuracy for Decision Tree Classifier =',accuracy2)
print('\n\nClassification_report :-\n\n',classification_report(Y_test, y_pred))
print('\n\nConfusion_Matrix :-\n')
plot_confusion_matrix(model2,X_test,Y_test)
plt.show()

"""Decision Tree method fit the dataset incredibly with  100% accuracy. It is very remarkable that there are only 1 false positives and 8 false negatives.

## RandomForestClassifier:
"""

from sklearn.ensemble import RandomForestClassifier

model3=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=42)

model3.fit(X_train,Y_train)

y_pred = model3.predict(X_test)

accuracy3=accuracy_score(Y_test, y_pred)*100
accuracy3

print('\nAccuracy for Random Forest Classifier =',accuracy3)
print('\n\nClassification_report :-\n\n',classification_report(Y_test, y_pred))
print('\n\nConfusion_Matrix :-\n')
plot_confusion_matrix(model3,X_test,Y_test)
plt.show()

"""AWESOME! 100% detection !! Random Forest wins this race!!

## GaussianNB:
"""

from sklearn.naive_bayes import GaussianNB

model4 = GaussianNB()

model4.fit(X_train,Y_train)

y_pred = model4.predict(X_test)

model4.score(X_train,Y_train)

model4.score(X_test,Y_test)

accuracy4 =accuracy_score(Y_test,y_pred)*100
accuracy4

print('\nAccuracy for Gaussian NB =',accuracy4)
print('\n\nClassification_report :-\n\n',classification_report(Y_test, y_pred))
print('\n\nConfusion_Matrix :-\n')
plot_confusion_matrix(model4,X_test,Y_test)
plt.show()

"""Here we have 94.8% accuracy, this is a very good model to use to visualize when it's a fraud and it's a good model to see if it's not a fraud too.

## KNeighborsClassifier:
"""

from sklearn.neighbors import KNeighborsClassifier

model5 = KNeighborsClassifier(n_neighbors=3)

model5.fit(X_train,Y_train)

model5.score(X_train,Y_train)*100

y_pred=model5.predict(X_test)

accuracy5=accuracy_score(Y_test, y_pred)*100
accuracy5

print('\nAccuracy for KNN =',accuracy5)
print('\n\nClassification_report :-\n\n',classification_report(Y_test, y_pred))
print('\n\nConfusion_Matrix :-\n')
plot_confusion_matrix(model5,X_test,Y_test)
plt.show()

"""here we have 98.4% accuracy, this is a very good model to use to visualize when it's a fraud and it's a good model to see if it's not a fraud too.

## Comparing Accuracy of All Models:
"""

print('Accuracy for Logistic Regression =',accuracy1)
print('Accuracy for Decision Tree Classifier =',accuracy2)
print('Accuracy for Random Forest Classifier =',accuracy3)
print('Accuracy for Gaussian NB =',accuracy4)
print('Accuracy for KNN =',accuracy5)

## Quick Summary Using Pandas_profiling

import pandas_profiling as pp
report = data.profile_report()
report

report.to_file("report.html")

"""## Conclusion:
In this project we can see that we have numerical variables and our categorical variables is already encoded, in our Data Visualization we can get some good insights
When we look to Machine Learning Models most of them has a good precision but not all can visualize if it's a fraud or no.
Other interesting thing to see is the most important variables in the Decision Tree Model, We can see that the most important variable of model is ratio_to_median_purchase_price, which shows that when it is a fraud, it is likely that the purchase price will have a large variance than normal
###  Best Models to use with Best Accuracy score:

- Decision Tree
- Random Forest
- KNN

###  Models with Good Accuracy:
- Naive Bayes
- Logistic Regression


#### The best model we have when we analyze the dataset, are the Random Forest and Decision Tree model ,in which we get 99.99% accuracy.

### Guided By- Shalini kumari
### Submitted By- Shalinee kumari
"""